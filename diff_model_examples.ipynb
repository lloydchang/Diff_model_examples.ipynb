{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1ySm6HYvALerDiGmk6g3pDz68V7fAtrQH\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Originated from Colab https://colab.research.google.com/drive/1ySm6HYvALerDiGmk6g3pDz68V7fAtrQH\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRrDKB8Ze4wM",
        "outputId": "bb6ffdf8-1242-412a-f058-6256f94112ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Iw4e-Ne-oe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CodeGenForCausalLM, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('CarperAI/diff-codegen-350m-v2')\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CodeGenForCausalLM.from_pretrained('CarperAI/diff-codegen-350m-v2').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl7wDvhSfv4l",
        "outputId": "3dafac61-551c-4947-e212-cf62ab107b9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/codegen/modeling_codegen.py:166: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n",
            "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<NME> parity.py\n",
            "<BEF> def parity(b1,b2,b3,b4):\n",
            "    \"\"\"Return binary parity of a sequence of input bits. Return 0 for even parity, 1 for odd parity.\"\"\"\n",
            "    bit_sum = sum([c1,b2,b3,b4])\n",
            "    return bit_sum % 2\n",
            "<MSG> Fixed bugs in sum\n",
            "<DFF> @@ -1,4 +1,4 @@\n",
            "-def parity(b1,b2,b3,b4):\n",
            "+def parity(b1,b2,b3,b4,b5):\n",
            "     \"\"\"Return binary parity of a sequence of input bits. Return 0 for even parity, 1 for odd parity.\"\"\"\n",
            "     bit_sum = sum([c1,b2,b3,b4])\n",
            "     return bit_sum % 2\n",
            "\\ No newline at end of file\n",
            "+def parity(b1,b2,b3,b4,b5,b6):\n",
            "+    \"\"\"Return binary parity of a sequence of input bits. Return 0 for even parity, 1 for odd parity.\"\"\"\n",
            "+    bit_sum = sum([c1,b2,b3,b4])\n",
            "+    return bit_sum % 2\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\n",
            "----------\n",
            "\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><NME> solution.py\n",
            "<BEF> class Solution:\n",
            "    \"\"\"Given an integer n, return the largest palindromic integer that can be represented as the product of two n-digits integers. Since the answer can be very large, return it modulo 1337.\"\"\"\n",
            "    def solution(self, n: int) -> int:\n",
            "\n",
            "<MSG> Finish the solution\n",
            "<DFF> @@ -1,4 +1,4 @@\n",
            "-class Solution:\n",
            "+class Solution:\n",
            "     \"\"\"Given an integer n, return the largest palindromic integer that can be represented as the product of two n-digits integers. Since the answer can be very large, return it modulo 1337.\"\"\"\n",
            "     def solution(self, n: int) -> int:\n",
            "\\ No newline at end of file\n",
            "+    \"\"\"\n",
            "+    Given an integer n, return the largest palindromic integer that can be represented as the product of two n-digits integers. Since the answer can be very large, return it modulo 1337.\n",
            "+    \"\"\"\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "+    \"\"\"\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "+    \"\"\"\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\n",
            "----------\n",
            "\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><NME> bug.py\n",
            "<BEF> def compare(x1, x2):\n",
            "    s1 = str(x1)\n",
            "    s2 = str(x1)\n",
            "    return s1 == s2\n",
            "<MSG> Fix bug: x2 instead of x1\n",
            "<DFF> @@ -1,1 +1,1 @@\n",
            "-def compare(x1, x2):\n",
            "+def compare(x1, x2, x3):\n",
            "     s1 = str(x1)\n",
            "     s2 = str(x1)\n",
            "     return s1 == s2\n",
            "\\ No newline at end of file\n",
            "+def compare(x1, x2, x3):\n",
            "+    s1 = str(x1)\n",
            "+    s2 = str(x1)\n",
            "+    return s1 == s2\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of file\n",
            "\\ No newline at end of\n",
            "\n",
            "----------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "'<NME> parity.py\\n'\n",
        "'<BEF> def parity(b1,b2,b3,b4):\\n'\n",
        "'    \"\"\"Return binary parity of a sequence of input bits. Return 0 for even parity, 1 for odd parity.\"\"\"\\n'\n",
        "'    bit_sum = sum([c1,b2,b3,b4])\\n'\n",
        "'    return bit_sum % 2\\n'\n",
        "'<MSG> Fixed bugs in sum\\n',\n",
        "'<NME> solution.py\\n'\n",
        "'<BEF> class Solution:\\n'\n",
        "'    \"\"\"Given an integer n, return the largest palindromic integer that can be represented as the product of two n-digits integ'\n",
        "'ers. Since the answer can be very large, return it modulo 1337.\"\"\"\\n'\n",
        "'    def solution(self, n: int) -> int:\\n\\n'\n",
        "'<MSG> Finish the solution\\n',\n",
        "'<NME> bug.py\\n'\n",
        "'<BEF> def compare(x1, x2):\\n'\n",
        "'    s1 = str(x1)\\n'\n",
        "'    s2 = str(x1)\\n'\n",
        "'    return s1 == s2\\n'\n",
        "'<MSG> Fix bug: x2 instead of x1\\n',\n",
        "]\n",
        "inputs = tokenizer(prompts, return_tensors='pt', padding=True).to(device)\n",
        "model.config.use_cache = True\n",
        "model_output = model.generate(**inputs, temperature=0.8, max_length=300)\n",
        "for i in range(model_output.size(0)):\n",
        "    print(tokenizer.decode(model_output[i]))\n",
        "    print('\\n' + '-'*10 + '\\n')  # Separating long generated texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thvzNpmahNNx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
